{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Simple tutorial for using TensorFlow to compute polynomial regression.\n",
    "\n",
    "Parag K. Mital, Jan. 2016\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maddoxw/.local/lib/python2.7/site-packages/matplotlib/figure.py:397: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/x/ZMJO+TGebkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f13014aaf10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% Let's create some toy data\n",
    "plt.ion()\n",
    "n_observations = 100\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "xs = np.linspace(-3, 3, n_observations)\n",
    "ys = np.sin(xs) + np.random.uniform(-0.5, 0.5, n_observations)\n",
    "ax.scatter(xs, ys)\n",
    "fig.show()\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %% tf.placeholders for the input and output of the network. Placeholders are\n",
    "# variables which we need to fill in when we are ready to compute the graph.\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %% Instead of a single factor and a bias, we'll create a polynomial function\n",
    "# of different polynomial degrees.  We will then learn the influence that each\n",
    "# degree of the input (X^0, X^1, X^2, ...) has on the final output (Y).\n",
    "Y_pred = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "for pow_i in range(1, 5):\n",
    "    W = tf.Variable(tf.random_normal([1]), name='weight_%d' % pow_i)\n",
    "    Y_pred = tf.add(tf.mul(tf.pow(X, pow_i), W), Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %% Loss function will measure the distance between our observations\n",
    "# and predictions and average over them.\n",
    "cost = tf.reduce_sum(tf.pow(Y_pred - Y, 2)) / (n_observations - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %% if we wanted to add regularization, we could add other terms to the cost,\n",
    "# e.g. ridge regression has a parameter controlling the amount of shrinkage\n",
    "# over the norm of activations. the larger the shrinkage, the more robust\n",
    "# to collinearity.\n",
    "# cost = tf.add(cost, tf.mul(1e-6, tf.global_norm([W])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %% Use gradient descent to optimize W,b\n",
    "# Performs a single step in the negative gradient\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.43746\n",
      "2.92743\n",
      "3.14274\n",
      "3.78016\n",
      "4.22986\n",
      "4.47306\n",
      "4.58016\n",
      "4.60922\n",
      "4.59611\n",
      "4.56096\n",
      "4.51462\n",
      "4.46278\n",
      "4.40845\n",
      "4.35316\n",
      "4.29771\n",
      "4.24252\n",
      "4.18777\n",
      "4.1336\n",
      "4.08003\n",
      "4.0271\n",
      "3.97481\n",
      "3.92316\n",
      "3.87214\n",
      "3.82176\n",
      "3.77199\n",
      "3.72285\n",
      "3.67431\n",
      "3.62638\n",
      "3.57905\n",
      "3.53231\n",
      "3.48615\n",
      "3.44056\n",
      "3.39555\n",
      "3.3511\n",
      "3.30721\n",
      "3.26386\n",
      "3.22107\n",
      "3.1788\n",
      "3.13708\n",
      "3.09588\n",
      "3.0552\n",
      "3.01504\n",
      "2.97538\n",
      "2.93623\n",
      "2.89758\n",
      "2.85942\n",
      "2.82175\n",
      "2.78455\n",
      "2.74783\n",
      "2.71158\n",
      "2.6758\n",
      "2.64048\n",
      "2.60561\n",
      "2.57119\n",
      "2.53721\n",
      "2.50368\n",
      "2.47058\n",
      "2.43791\n",
      "2.40566\n",
      "2.37383\n",
      "2.34242\n",
      "2.31141\n",
      "2.28081\n",
      "2.25061\n",
      "2.22081\n",
      "2.1914\n",
      "2.16237\n",
      "2.13373\n",
      "2.10546\n",
      "2.07757\n",
      "2.05004\n",
      "2.02288\n",
      "1.99608\n",
      "1.96963\n",
      "1.94353\n",
      "1.91778\n",
      "1.89237\n",
      "1.8673\n",
      "1.84257\n",
      "1.81816\n",
      "1.79409\n",
      "1.77033\n",
      "1.74689\n",
      "1.72377\n",
      "1.70095\n",
      "1.67844\n",
      "1.65624\n",
      "1.63433\n",
      "1.61272\n",
      "1.5914\n",
      "1.57037\n",
      "1.54962\n",
      "1.52916\n",
      "1.50897\n",
      "1.48905\n",
      "1.46941\n",
      "1.45003\n",
      "1.43091\n",
      "1.41206\n",
      "1.39346\n",
      "1.37511\n",
      "1.35702\n",
      "1.33917\n",
      "1.32156\n",
      "1.3042\n",
      "1.28707\n",
      "1.27018\n",
      "1.25351\n",
      "1.23708\n",
      "1.22087\n",
      "1.20489\n",
      "1.18912\n",
      "1.17357\n",
      "1.15823\n",
      "1.14311\n",
      "1.12819\n",
      "1.11348\n",
      "1.09897\n",
      "1.08466\n",
      "1.07055\n",
      "1.05664\n",
      "1.04291\n",
      "1.02938\n",
      "1.01603\n",
      "1.00287\n",
      "0.989887\n",
      "0.977086\n",
      "0.964463\n",
      "0.952015\n",
      "0.939739\n",
      "0.927634\n",
      "0.915697\n",
      "0.903926\n",
      "0.892318\n",
      "0.880871\n",
      "0.869585\n",
      "0.858454\n",
      "0.847479\n",
      "0.836657\n",
      "0.825986\n",
      "0.815463\n",
      "0.805087\n",
      "0.794855\n",
      "0.784767\n",
      "0.774819\n",
      "0.765011\n",
      "0.755338\n",
      "0.745802\n",
      "0.736398\n",
      "0.727127\n",
      "0.717985\n",
      "0.70897\n",
      "0.700083\n",
      "0.691319\n",
      "0.682678\n",
      "0.674159\n",
      "0.665759\n",
      "0.657476\n",
      "0.64931\n",
      "0.641258\n",
      "0.63332\n",
      "0.625492\n",
      "0.617775\n",
      "0.610167\n",
      "0.602665\n",
      "0.595268\n",
      "0.587976\n",
      "0.580786\n",
      "0.573697\n",
      "0.566708\n",
      "0.559818\n",
      "0.553024\n",
      "0.546325\n",
      "0.539722\n",
      "0.533211\n",
      "0.526793\n",
      "0.520464\n",
      "0.514225\n",
      "0.508074\n",
      "0.502009\n",
      "0.49603\n",
      "0.490136\n",
      "0.484325\n",
      "0.478595\n",
      "0.472947\n",
      "0.467378\n",
      "0.461888\n",
      "0.456476\n",
      "0.45114\n",
      "0.445879\n",
      "0.440693\n",
      "0.43558\n",
      "0.43054\n",
      "0.425571\n",
      "0.420673\n",
      "0.415844\n",
      "0.411083\n",
      "0.406389\n",
      "0.401762\n",
      "0.3972\n",
      "0.392703\n",
      "0.38827\n",
      "0.3839\n",
      "0.379591\n",
      "0.375344\n",
      "0.371156\n",
      "0.367029\n",
      "0.36296\n",
      "0.358949\n",
      "0.354995\n",
      "0.351096\n",
      "0.347254\n",
      "0.343465\n",
      "0.339731\n",
      "0.336049\n",
      "0.33242\n",
      "0.328843\n",
      "0.325316\n",
      "0.32184\n",
      "0.318413\n",
      "0.315034\n",
      "0.311704\n",
      "0.308422\n",
      "0.305185\n",
      "0.301995\n",
      "0.298851\n",
      "0.295751\n",
      "0.292696\n",
      "0.289684\n",
      "0.286715\n",
      "0.283788\n",
      "0.280903\n",
      "0.278059\n",
      "0.275256\n",
      "0.272493\n",
      "0.269769\n",
      "0.267084\n",
      "0.264437\n",
      "0.261828\n",
      "0.259256\n",
      "0.256721\n",
      "0.254222\n",
      "0.25176\n",
      "0.249331\n",
      "0.246938\n",
      "0.244579\n",
      "0.242254\n",
      "0.239962\n",
      "0.237703\n",
      "0.235477\n",
      "0.233282\n",
      "0.231118\n",
      "0.228986\n",
      "0.226884\n",
      "0.224813\n",
      "0.22277\n",
      "0.220758\n",
      "0.218774\n",
      "0.216818\n",
      "0.214891\n",
      "0.212991\n",
      "0.211118\n",
      "0.209272\n",
      "0.207453\n",
      "0.20566\n",
      "0.203892\n",
      "0.202151\n",
      "0.200434\n",
      "0.198742\n",
      "0.197074\n",
      "0.19543\n",
      "0.19381\n",
      "0.192213\n",
      "0.190639\n",
      "0.189088\n",
      "0.187559\n",
      "0.186052\n",
      "0.184567\n",
      "0.183103\n",
      "0.18166\n",
      "0.180238\n",
      "0.178837\n",
      "0.177455\n",
      "0.176094\n",
      "0.174752\n",
      "0.17343\n",
      "0.172127\n",
      "0.170843\n",
      "0.169577\n",
      "0.168329\n",
      "0.1671\n",
      "0.165888\n",
      "0.164695\n",
      "0.163518\n",
      "0.162358\n",
      "0.161215\n",
      "0.160088\n",
      "0.158978\n",
      "0.157884\n",
      "0.156807\n",
      "0.155744\n",
      "0.154697\n",
      "0.153665\n",
      "0.152649\n",
      "0.151647\n",
      "0.150659\n",
      "0.149686\n",
      "0.148727\n",
      "0.147782\n",
      "0.146851\n",
      "0.145933\n",
      "0.145028\n",
      "0.144137\n",
      "0.143259\n",
      "0.142394\n",
      "0.141541\n",
      "0.1407\n",
      "0.139872\n",
      "0.139056\n",
      "0.138252\n",
      "0.137459\n",
      "0.136679\n",
      "0.135909\n",
      "0.135151\n",
      "0.134405\n",
      "0.133669\n",
      "0.132944\n",
      "0.132229\n",
      "0.131525\n",
      "0.130831\n",
      "0.130147\n",
      "0.129474\n",
      "0.12881\n",
      "0.128156\n",
      "0.127512\n",
      "0.126877\n",
      "0.126251\n",
      "0.125635\n",
      "0.125028\n",
      "0.12443\n",
      "0.12384\n",
      "0.12326\n",
      "0.122687\n",
      "0.122124\n",
      "0.121568\n",
      "0.121021\n",
      "0.120482\n",
      "0.119951\n",
      "0.119427\n",
      "0.118912\n",
      "0.118404\n",
      "0.117904\n",
      "0.117411\n",
      "0.116925\n",
      "0.116447\n",
      "0.115976\n",
      "0.115511\n",
      "0.115054\n",
      "0.114603\n",
      "0.11416\n",
      "0.113722\n",
      "0.113292\n",
      "0.112867\n",
      "0.112449\n",
      "0.112038\n",
      "0.111632\n",
      "0.111233\n",
      "0.110839\n",
      "0.110452\n",
      "0.11007\n",
      "0.109694\n",
      "0.109323\n",
      "0.108959\n",
      "0.108599\n",
      "0.108245\n",
      "0.107896\n",
      "0.107553\n",
      "0.107215\n",
      "0.106882\n",
      "0.106554\n",
      "0.106231\n",
      "0.105912\n",
      "0.105599\n",
      "0.10529\n",
      "0.104986\n",
      "0.104687\n",
      "0.104392\n",
      "0.104102\n",
      "0.103816\n",
      "0.103534\n",
      "0.103257\n",
      "0.102984\n",
      "0.102715\n",
      "0.10245\n",
      "0.102189\n",
      "0.101932\n",
      "0.101679\n",
      "0.10143\n",
      "0.101185\n",
      "0.100944\n",
      "0.100706\n",
      "0.100472\n",
      "0.100241\n",
      "0.100015\n",
      "0.0997912\n",
      "0.0995712\n",
      "0.0993546\n",
      "0.0991413\n",
      "0.0989314\n",
      "0.0987248\n",
      "0.0985214\n",
      "0.098321\n",
      "0.0981237\n",
      "0.0979295\n",
      "0.0977384\n",
      "0.0975503\n",
      "0.0973652\n",
      "0.0971828\n",
      "0.0970033\n",
      "0.0968267\n",
      "0.0966527\n",
      "0.0964816\n",
      "0.0963131\n",
      "0.0961474\n",
      "0.0959842\n",
      "0.0958235\n",
      "0.0956655\n",
      "0.0955099\n",
      "0.0953568\n",
      "0.095206\n",
      "0.0950577\n",
      "0.0949118\n",
      "0.0947681\n",
      "0.0946268\n",
      "0.0944876\n",
      "0.0943508\n",
      "0.0942162\n",
      "0.0940837\n",
      "0.0939533\n",
      "0.093825\n",
      "0.0936987\n",
      "0.0935744\n",
      "0.0934522\n",
      "0.093332\n",
      "0.0932137\n",
      "0.0930974\n",
      "0.0929829\n",
      "0.0928703\n",
      "0.0927595\n",
      "0.0926504\n",
      "0.0925431\n",
      "0.0924377\n",
      "0.092334\n",
      "0.092232\n",
      "0.0921316\n",
      "0.0920329\n",
      "0.0919358\n",
      "0.0918404\n",
      "0.0917466\n",
      "0.0916542\n",
      "0.0915635\n",
      "0.0914741\n",
      "0.0913863\n",
      "0.0913\n",
      "0.0912151\n",
      "0.0911317\n",
      "0.0910496\n",
      "0.0909689\n",
      "0.0908896\n",
      "0.0908116\n",
      "0.090735\n",
      "0.0906596\n",
      "0.0905856\n",
      "0.0905127\n",
      "0.0904411\n",
      "0.0903707\n",
      "0.0903016\n",
      "0.0902337\n",
      "0.0901669\n",
      "0.0901012\n",
      "0.0900367\n",
      "0.0899733\n",
      "0.089911\n",
      "0.08985\n",
      "0.0897898\n",
      "0.0897307\n",
      "0.0896727\n",
      "0.0896157\n",
      "0.0895597\n",
      "0.0895047\n",
      "0.0894506\n",
      "0.0893975\n",
      "0.0893454\n",
      "0.0892942\n",
      "0.089244\n",
      "0.0891946\n",
      "0.0891461\n",
      "0.0890985\n",
      "0.0890517\n",
      "0.0890059\n",
      "0.0889608\n",
      "0.0889166\n",
      "0.0888732\n",
      "0.0888306\n",
      "0.0887888\n",
      "0.0887477\n",
      "0.0887074\n",
      "0.0886679\n",
      "0.0886291\n",
      "0.0885911\n",
      "0.0885537\n",
      "0.0885171\n",
      "0.0884811\n",
      "0.0884459\n",
      "0.0884113\n",
      "0.0883774\n",
      "0.0883442\n",
      "0.0883115\n",
      "0.0882796\n",
      "0.0882482\n",
      "0.0882175\n",
      "0.0881873\n",
      "0.0881578\n",
      "0.0881288\n",
      "0.0881005\n",
      "0.0880727\n",
      "0.0880454\n",
      "0.0880188\n",
      "0.0879926\n",
      "0.087967\n",
      "0.0879419\n",
      "0.0879174\n"
     ]
    }
   ],
   "source": [
    "# %% We create a session to use the graph\n",
    "n_epochs = 1000\n",
    "with tf.Session() as sess:\n",
    "    # Here we tell tensorflow that we want to initialize all\n",
    "    # the variables in the graph so we can use them\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # Fit all training data\n",
    "    prev_training_cost = 0.0\n",
    "    for epoch_i in range(n_epochs):\n",
    "        for (x, y) in zip(xs, ys):\n",
    "            sess.run(optimizer, feed_dict={X: x, Y: y})\n",
    "\n",
    "        training_cost = sess.run(\n",
    "            cost, feed_dict={X: xs, Y: ys})\n",
    "        print(training_cost)\n",
    "\n",
    "        if epoch_i % 100 == 0:\n",
    "            ax.plot(xs, Y_pred.eval(\n",
    "                feed_dict={X: xs}, session=sess),\n",
    "                    'k', alpha=epoch_i / n_epochs)\n",
    "            fig.show()\n",
    "            plt.draw()\n",
    "\n",
    "        # Allow the training to quit if we've reached a minimum\n",
    "        if np.abs(prev_training_cost - training_cost) < 0.000001:\n",
    "            break\n",
    "        prev_training_cost = training_cost\n",
    "ax.set_ylim([-3, 3])\n",
    "fig.show()\n",
    "plt.waitforbuttonpress()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
